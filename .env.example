# GraphRAG Environment Configuration

# ====== Database Connection Settings ======
# Qdrant vector database settings
QDRANT_HOST=localhost
QDRANT_PORT=6333

# Neo4j graph database settings
NEO4J_URI=bolt://localhost:7687
NEO4J_USERNAME=neo4j
NEO4J_PASSWORD=password

# ====== LLM Provider Settings ======
# Set the LLM provider to use (openai or ollama)
MODEL_PROVIDER=openai

# ====== OpenAI Settings ======
# API key for OpenAI (required if MODEL_PROVIDER=openai)
OPENAI_API_KEY=your_openai_api_key
# Model to use for question answering
OPENAI_LLM_MODEL=gpt-4o-mini
# Model to use for embeddings
OPENAI_EMBEDDING_MODEL=text-embedding-3-small

# ====== Ollama Settings ======
# Host for Ollama API (only needed if MODEL_PROVIDER=ollama)
OLLAMA_HOST=localhost
# Port for Ollama API
OLLAMA_PORT=11434
# Model to use for question answering with Ollama
LLM_MODEL=qwen2.5:3b
# Model to use for embeddings with Ollama
EMBEDDING_MODEL=nomic-embed-text

# ====== Collection Settings ======
# Name of the collection in Qdrant
COLLECTION_NAME=graphRAGstoreds
# Vector dimension for the embeddings (1536 for OpenAI, 768 for most Ollama models)
VECTOR_DIMENSION=1536

# ====== Performance Settings ======
# Enable parallel processing for data ingestion
PARALLEL_PROCESSING=true
# Number of parallel workers
MAX_WORKERS=4
# Batch size for database operations
BATCH_SIZE=100
# Size of text chunks for processing
CHUNK_SIZE=5000
# Enable streaming responses from LLM
USE_STREAMING=true
# Request timeout in seconds
REQUEST_TIMEOUT=60 